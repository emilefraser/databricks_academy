# Databricks notebook source
# MAGIC %md
# MAGIC # Working with Delta Live Tables
# MAGIC 
# MAGIC This is a companion notebook that loads new data and explores the results generated by scheduling one of the included [Python]($./DLT-python) or [SQL]($./DLT-sql) Delta Live Tables (DLT) example notebooks.
# MAGIC 
# MAGIC Up-to-date documentation on Delta Live Tables can be found [here](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
# MAGIC 
# MAGIC ## Learning Objectives
# MAGIC By the end of this lesson, students will be able to:
# MAGIC * Describe how new data propagates through pipelines built with Delta Live Tables
# MAGIC * Use the DLT UI to configure pipelines from Python or SQL notebooks
# MAGIC * Run pipeline updates in `triggered` execution mode
# MAGIC * Modify a pipeline to run in `continuous` execution mode

# COMMAND ----------

# MAGIC %md
# MAGIC ## About this Notebook
# MAGIC 
# MAGIC As you proceed, keep in mind that this notebook is not actually part of the Delta Live Tables process. Rather, this notebook is a tool used to configure an environment in which to run a demo and to provide instructions for various steps.
# MAGIC 
# MAGIC Specifically, this notebook contains code that does the following:
# MAGIC * Creates a data directory for storing all demo data and metadata
# MAGIC * Simulates new data arriving in cloud object storage
# MAGIC * Queries tables produced by the scheduled DLT pipeline
# MAGIC * Displays log information recorded by DLT
# MAGIC * Deletes all data and tables created by this demo

# COMMAND ----------

# MAGIC %md
# MAGIC ## Setup the Environment
# MAGIC Begin by running the following cell. Note that the `$mode` is currently configured to recursively drop the data in the source as well as the destination; this is to allow you to reuse this demo to show the end-to-end process of scheduling a new DLT pipeline and show new data processing.
# MAGIC 
# MAGIC **NOTE**: This notebook can be used alongside either the SQL or Python notebook provided, but this setup script should be run before trying to deploy a new pipeline against the data being loaded.

# COMMAND ----------

# MAGIC %run ./Includes/setup $mode="reset"

# COMMAND ----------

# MAGIC %md
# MAGIC The `NewFile` helper function was declared during the setup. We'll use it to write out batches of JSON data to a location on the root DBFS.
# MAGIC 
# MAGIC This data will be consumed by our pipeline in the same way we can use an external cloud object storage location.

# COMMAND ----------

NewFile.arrival()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Explore the DLT Notebook
# MAGIC 
# MAGIC As mentioned previously, [Python]($./DLT-python) or [SQL]($./DLT-sql) companion notebooks are provided with complimentary DLT syntax. Note that while slight differences exist between these notebooks, the core logic being applied to our data is identical.
# MAGIC 
# MAGIC Before proceeding, review the syntax in your chosen notebook.
# MAGIC 
# MAGIC **NOTE**: The first code cell in each notebook has a string indicating `"<YOUR_USERNAME_HERE>"`. Run the next cell to display your current username and copy and paste this. (Your username is used by the setup script to make sure the data directories used are isolated from other users in your workspace.)

# COMMAND ----------

username

# COMMAND ----------

# MAGIC %md
# MAGIC ## Configure your Pipeline
# MAGIC 
# MAGIC Start by clicking on the **Jobs** button on the left navigation bar. A **Delta Live Tables** tab should be visible at the top of the page. (Note that this feature is still in a gated public preview as of this recording; [you can use this link to request access to this feature](https://databricks.com/p/product-delta-live-tables).)
# MAGIC 
# MAGIC Navigate here click **Create Pipeline** to get started.
# MAGIC 
# MAGIC **Pipeline Name** and **Notebook Path** are the two required values. Make sure that you use the notebook selector to choose either the **DLT-python** or **DLT-sql** notebooks stored parallel to this notebook.
# MAGIC 
# MAGIC The cell below prints out the value of your `storage_location` path. Copy and past this value as the **Storage Location** when configuring your pipeline.
# MAGIC 
# MAGIC The **Storage Location** controls the location that logs, data, and metadata from your pipeline are recorded. In production, you'll want to make sure this points to a secure external object storage location.

# COMMAND ----------

storage_location.split(':')[1]

# COMMAND ----------

# MAGIC %md
# MAGIC Make sure you've selected **Triggered** execution mode.
# MAGIC 
# MAGIC Finally, click **Create**.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Run your Pipeline
# MAGIC 
# MAGIC If available, configure your pipeline to run in **Development** mode. This will allow you to reuse the same cluster between runs and will turn off automatic retries on job failure.
# MAGIC 
# MAGIC Click **Start** to begin the first update to your table.
# MAGIC 
# MAGIC Delta Live Tables will automatically deploy all the necessary infrastructure and resolve the dependencies between all datasets.
# MAGIC 
# MAGIC **NOTE**: The first table update make take several minutes as relationships are resolved and infrastructure deploys.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Troubleshooting Code in Development Mode
# MAGIC 
# MAGIC Delta Live Tables is in active development, and error messages are improving all the time.
# MAGIC 
# MAGIC Because relationships between tables are mapped as a DAG, error messages will often indicate that a dataset isn't found.
# MAGIC 
# MAGIC Let's consider our DAG below:
# MAGIC 
# MAGIC <img src="https://files.training.databricks.com/images/dlt_dag.png" width="400">
# MAGIC 
# MAGIC If the error message `Dataset not found: 'recordings_parsed'` is raised, there may be several culprits:
# MAGIC 1. The logic defining `recordings_parsed` is invalid
# MAGIC 1. There is an error reading from `recordings_bronze`
# MAGIC 1. A typo exists in either `recordings_parsed` or `recordings_bronze`
# MAGIC 
# MAGIC The safest way to identify the culprit is to iteratively add table/view definitions back into your DAG starting from your initial ingestion tables. You can simply comment out later table/view definitions and uncomment these between runs.

# COMMAND ----------

# MAGIC %md
# MAGIC ## Query your Tables
# MAGIC Because we used the path assigned to our `storage_location` variable as the storage location for our pipeline, all tables will be written there.
# MAGIC 
# MAGIC We can query tables and views using the following function. This will only work once your pipeline has successfully run the first time.

# COMMAND ----------

def query_table(table_name):
    return spark.sql(f"SELECT * FROM delta.`{storage_location}/tables/{table_name}`")

# COMMAND ----------

# MAGIC %md
# MAGIC Let's query our gold table to see our aggregate results. Note the number of rows.

# COMMAND ----------

display(query_table("daily_patient_avg"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Query your Logs
# MAGIC 
# MAGIC Logs start getting recorded as soon as you create your pipeline and will track all updates, actions, and modifications.
# MAGIC 
# MAGIC The `storage_location` directory specified during pipeline creation will contain checkpoint and autoloader state files, tables, and a `system` directory.

# COMMAND ----------

dbutils.fs.ls(storage_location)

# COMMAND ----------

# MAGIC %md
# MAGIC This `system` directory will automatically capture all events associated with our pipeline.

# COMMAND ----------

dbutils.fs.ls(storage_location + "/system")

# COMMAND ----------

# MAGIC %md
# MAGIC These logs are stored as a Delta table, so we can easily load these in and review information.

# COMMAND ----------

logDF = spark.read.load(storage_location + "/system/events")
logDF.createOrReplaceTempView("logs")
display(logDF)

# COMMAND ----------

# MAGIC %md
# MAGIC If you're only interested in those rows that indicate data processing metrics, you can run the following filter.

# COMMAND ----------

display(logDF.filter("details:flow_progress:metrics IS NOT NULL"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Update your Tables
# MAGIC Run the following cell to land new data in the source directory (simulating a new batch of data being loaded by an outside system).

# COMMAND ----------

NewFile.arrival()

# COMMAND ----------

# MAGIC %md
# MAGIC Because our pipeline is currently running in an incremental batch mode, we'll need to manually trigger it again. Click **Start** back in the Pipelines GUI and wait until all data has been processed, then run the following cell to confirm new records have arrived.

# COMMAND ----------

display(query_table("daily_patient_avg"))

# COMMAND ----------

# MAGIC %md
# MAGIC ## Run in Continous Processing Mode
# MAGIC 
# MAGIC Before landing more data, change the pipeline to use `continuous` execution by editting the JSON in **Edit Settings** of the UI and setting `"continuous": true`. (**NOTE**: This is the same as selecting **Continuous** execution mode from the UI during initial pipeline configuration).
# MAGIC 
# MAGIC This configures the pipeline to always be looking for new data and to execute updates whenever new records arrive.
# MAGIC 
# MAGIC Continuous execution with incremental live tables provides near-real time stream processing.
# MAGIC 
# MAGIC **NOTE**: There are a [number of configurations that can be configured in this JSON file](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-configuration.html#configuration-settings); the UI for Delta Live Tables is still in development and more of these configurations will be available natively in the UI in the near future.

# COMMAND ----------

NewFile.arrival()

# COMMAND ----------

# MAGIC %md
# MAGIC Our `NewFile` class has 12 batches of data, so feel free to trigger a few more batches manually and track progress using the UI or querying tables.
# MAGIC 
# MAGIC **Note**: in continuous execution mode, our pipeline will remain on indefinitely waiting for new data.

# COMMAND ----------

display(query_table("daily_patient_avg"))

# COMMAND ----------

query_table("recordings_enriched").count()

# COMMAND ----------

# MAGIC %md
# MAGIC ## Examine Batch Metrics
# MAGIC 
# MAGIC When updates are successful, we can track the logs through the `details` column using the query below.

# COMMAND ----------

successDF = logDF.filter("details:flow_progress:metrics IS NOT NULL")
display(successDF)

# COMMAND ----------

# MAGIC %md
# MAGIC We can zoom in to see metrics on each table update. Note that in continuous execution mode, the flow status will indicate "RUNNING" for successfully processed batches, while in triggered execution mode you will see "COMPLETED".

# COMMAND ----------

display(successDF.select("timestamp", "origin.flow_name", "details").orderBy(F.desc("timestamp")))

# COMMAND ----------

# MAGIC %md
# MAGIC Feel free to explore other log information using the cell below.

# COMMAND ----------

# MAGIC %sql
# MAGIC SELECT * FROM logs

# COMMAND ----------

# MAGIC %md
# MAGIC ## Summary
# MAGIC By completing this lesson, you should now be comfortable:
# MAGIC * Configuring DLT pipelines from Python and SQL notebooks
# MAGIC * Executing pipeline updates using `continuous` or `triggered` execution modes
# MAGIC * Describing how new data propagates through Delta Live Tables
# MAGIC 
# MAGIC If you're looking for additional information, make sure to [read the docs](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).

# COMMAND ----------

# MAGIC %md
# MAGIC ## Cleaning Up
# MAGIC **Make sure you stop your continuous pipeline.** Leaving a continuous pipeline on will keep resources live and continously incur costs.
# MAGIC 
# MAGIC To fully clean up after yourself, delete your pipeline. This should automatically delete the 2 associated jobs. (Jobs will be named: **Pipelines - `<your_pipeline_name>`** and **Pipeline Maintenance - `<your_pipeline_name>`**).
# MAGIC 
# MAGIC The following cell can be run to delete the data files, tables, and logs associated with this demo.

# COMMAND ----------

# MAGIC %run ./Includes/setup $mode="cleanup"
